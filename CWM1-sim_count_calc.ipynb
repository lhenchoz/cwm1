{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as stats\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "def accident_prob_front(reaction_time,cars_per_day):\n",
    "    seconds_per_year = 86400 * 365\n",
    "    danger_time_per_car = reaction_time\n",
    "    cars_per_year = cars_per_day * 365\n",
    "    danger_time_total = danger_time_per_car * cars_per_year\n",
    "    danger_prob = (danger_time_total / seconds_per_year)\n",
    "\n",
    "    return danger_prob"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [],
   "source": [
    "def net_test(net_status, weight_in_net, impact_energy):\n",
    "    if net_status == True:\n",
    "        return True\n",
    "    elif weight_in_net > 2000:\n",
    "        if impact_energy > 500:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif weight_in_net <= 2000:\n",
    "        if impact_energy > 1000:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [],
   "source": [
    "def accident_prob_top(v_km_h, laenge_auto, cars_per_day):\n",
    "    seconds_per_year = 86400 * 365\n",
    "    v_m_s = (v_km_h / 3.6)\n",
    "    danger_time_per_car = (laenge_auto / v_m_s)\n",
    "    cars_per_year = cars_per_day * 365\n",
    "    danger_time_total = danger_time_per_car * cars_per_year\n",
    "    danger_prob = (danger_time_total / seconds_per_year)\n",
    "\n",
    "    return danger_prob"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0033333333333333335"
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accident_prob_top(60,4,1200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "accident_prob_tops = accident_prob_top(60,4.4,1200)\n",
    "accident_prob_fronts = accident_prob_front(1.5,1200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rockfall_sim(sim_count,df1,df2):\n",
    "    param = stats.gamma.fit(df1['Mass_(kg)'])\n",
    "    zone1_mass = stats.gamma.rvs(*param, size = sim_count)\n",
    "    param = stats.norm.fit(df1[\"Velocity_(m/s)\"])\n",
    "    zone1_velocity = stats.norm.rvs(*param, size = sim_count)\n",
    "    param = stats.expon.fit(df1[\"Time_delta_(h)\"])\n",
    "    zone1_time_delta = stats.expon.rvs(*param, size = sim_count)\n",
    "    param = stats.gamma.fit(df2['Mass_(kg)'])\n",
    "    zone2_mass = stats.gamma.rvs(*param, size = sim_count)\n",
    "    param = stats.norm.fit(df2[\"Velocity_(m/s)\"])\n",
    "    zone2_velocity = stats.norm.rvs(*param, size = sim_count)\n",
    "    param = stats.expon.fit(df2[\"Time_delta_(h)\"])\n",
    "    zone2_time_delta = stats.expon.rvs(*param, size = sim_count)\n",
    "\n",
    "    df1_sim = pd.DataFrame()\n",
    "\n",
    "    df1_sim.insert(0, \"Mass_(kg)\", zone1_mass)\n",
    "    df1_sim.insert(1, \"Velocity_(m/s)\", zone1_velocity)\n",
    "    df1_sim.insert(2, \"Energy_(kJ)\", \"\")\n",
    "    df1_sim.insert(3, \"Time_delta_(h)\", zone1_time_delta)\n",
    "    df1_sim[\"Energy_(kJ)\"] = (0.5 * df1_sim[\"Mass_(kg)\"] * (df1_sim[\"Velocity_(m/s)\"] ** 2)) / 1000\n",
    "    df1_sim[\"Time_delta_(h)\"] = zone1_time_delta.round(2)\n",
    "    df1_sim[\"Time_delta_(h)\"][0] = 0\n",
    "    if 'Time_total' not in df1_sim.columns:\n",
    "        df1_sim.insert(4, \"Time_total\", \"\")\n",
    "    df2_sim = pd.DataFrame()\n",
    "    df2_sim.insert(0, \"Mass_(kg)\", zone2_mass)\n",
    "    df2_sim.insert(1, \"Velocity_(m/s)\", zone2_velocity)\n",
    "    df2_sim.insert(2, \"Energy_(kJ)\", \"\")\n",
    "    df2_sim.insert(3, \"Time_delta_(h)\", zone2_time_delta)\n",
    "    df2_sim[\"Energy_(kJ)\"] = (0.5 * df2_sim[\"Mass_(kg)\"] * (df2_sim[\"Velocity_(m/s)\"] ** 2)) / 1000\n",
    "    df2_sim[\"Time_delta_(h)\"] = zone2_time_delta.round(2)\n",
    "    df2_sim[\"Time_delta_(h)\"][0] = 0\n",
    "    if 'Time_total' not in df2_sim.columns:\n",
    "        df2_sim.insert(4, \"Time_total\", \"\")\n",
    "\n",
    "    df1_sim[\"Time_total\"] = np.cumsum(df1_sim[\"Time_delta_(h)\"])\n",
    "    df2_sim[\"Time_total\"] = np.cumsum(df2_sim[\"Time_delta_(h)\"])\n",
    "    df2_sim = df2_sim.loc[df2_sim['Time_total'] <= sum(df1_sim[\"Time_delta_(h)\"])]\n",
    "    df_sim = pd.concat([df1_sim,df2_sim],ignore_index= True)\n",
    "    df_sim = df_sim.sort_values([\"Time_total\"],ignore_index=True)\n",
    "    df_sim[\"Time_delta_(h)\"] = df_sim[\"Time_total\"].diff()\n",
    "    df_sim = df_sim.drop(\"Time_total\", axis=1)\n",
    "    df_sim[\"Time_delta_(h)\"][0] = 0\n",
    "\n",
    "    time_interval = 24\n",
    "    current_delta = 0\n",
    "    hours_total = 0\n",
    "    weight_in_net = 0\n",
    "    net_status = False\n",
    "    incident = 0\n",
    "    stones_in_net = 0\n",
    "    sim_stones = len(df_sim)\n",
    "    for i in range(int(sim_stones)):\n",
    "        iterated_delta = df_sim[\"Time_delta_(h)\"][i]\n",
    "        iterated_energy = df_sim[\"Energy_(kJ)\"][i]\n",
    "        hours_total += iterated_delta\n",
    "        if (iterated_delta > time_interval):\n",
    "            if net_status:\n",
    "                incident += stones_in_net\n",
    "            stones_in_net = 1\n",
    "            net_status = net_test(False,0,iterated_energy)\n",
    "            weight_in_net = df_sim[\"Mass_(kg)\"][i]\n",
    "            iterated_delta = iterated_delta % time_interval\n",
    "            if iterated_delta > (time_interval - current_delta):\n",
    "                current_delta = abs((time_interval - current_delta)-iterated_delta)\n",
    "            elif iterated_delta <= (time_interval - current_delta):\n",
    "                current_delta = current_delta + iterated_delta\n",
    "        elif iterated_delta <= time_interval:\n",
    "            if iterated_delta > (time_interval - current_delta):\n",
    "                if net_status:\n",
    "                    incident += stones_in_net\n",
    "                stones_in_net = 1\n",
    "                net_status = net_test(False,0,iterated_energy)\n",
    "                weight_in_net = df_sim[\"Mass_(kg)\"][i]\n",
    "                current_delta = abs((time_interval - current_delta)-iterated_delta)\n",
    "            elif iterated_delta <= (time_interval - current_delta):\n",
    "                stones_in_net += 1\n",
    "                net_status = net_test(net_status,weight_in_net,iterated_energy)\n",
    "                weight_in_net += df_sim[\"Mass_(kg)\"][i]\n",
    "                current_delta = current_delta + iterated_delta\n",
    "    amount_sim_years = hours_total / 8760\n",
    "    stones_per_year = (sim_stones / amount_sim_years)\n",
    "    incident_prob_per_stone = incident/sim_stones\n",
    "    incident_prob_per_year = incident_prob_per_stone * stones_per_year\n",
    "    stone_hit_top_prob = accident_prob_tops * incident_prob_per_year\n",
    "    stone_hit_front_prob = accident_prob_fronts * incident_prob_per_year\n",
    "    stone_hit_total_prob = stone_hit_top_prob + stone_hit_front_prob\n",
    "    final_prob = (stone_hit_total_prob * 1.6)\n",
    "    return final_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [],
   "source": [
    "def open_csv_to_df(directory):\n",
    "    df = pd.read_csv(directory)\n",
    "    df = df.iloc[0:,:4]\n",
    "    df.columns = [\"Date\", \"Time\", \"Mass_(kg)\", \"Velocity_(m/s)\"]\n",
    "    df = df.sort_values(['Date', 'Time']).reset_index(drop=True)\n",
    "    df = df.dropna()\n",
    "    df[\"Date\"] = pd.to_datetime(df.Date.astype(str) + \" \" +  df.Time.astype(str), format = '%Y-%m-%d %H:%M')\n",
    "    df.rename(columns = {'Date':'Datetime'}, inplace=True)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "def calc_time_delta(df):\n",
    "\n",
    "    # Neue Spalte erstellen\n",
    "    df.insert(3, \"Time_delta_(h)\", \"\")\n",
    "\n",
    "    # Zeitabstand berechnen\n",
    "    for i in range(len(df)-1):\n",
    "        date1 = df.iloc[i,0]\n",
    "        date2 = df.iloc[i+1,0]\n",
    "        time_delta = date2 - date1\n",
    "        time_delta = (time_delta.days*24) + (time_delta.seconds//3600)\n",
    "        df.iloc[i+1,3] = time_delta\n",
    "\n",
    "    # Erster Eintrag mit Null ersetzen\n",
    "    df.iloc[0,3] = 0\n",
    "\n",
    "    # Datentyp zu Integer Ã¤ndern\n",
    "    df['Time_delta_(h)'] = df['Time_delta_(h)'].astype('int')\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "df_area1 = open_csv_to_df(\"./out_1.csv\")\n",
    "df_area2 = open_csv_to_df(\"./out_2.csv\")\n",
    "df_area1 = calc_time_delta(df_area1)\n",
    "df_area2 = calc_time_delta(df_area2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "data": {
      "text/plain": "0.00272168415250335"
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rockfall_sim(100000,df_area1,df_area2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10010\n",
      "20010\n",
      "30010\n",
      "40010\n",
      "50010\n",
      "60010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[237], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m,\u001B[38;5;241m1000000\u001B[39m,\u001B[38;5;241m10000\u001B[39m):\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(i)\n\u001B[1;32m----> 4\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mrockfall_sim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_area1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_area2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     stone_probs\u001B[38;5;241m.\u001B[39mappend((result,i))\n",
      "Cell \u001B[1;32mIn[232], line 56\u001B[0m, in \u001B[0;36mrockfall_sim\u001B[1;34m(sim_count, df1, df2)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mint\u001B[39m(sim_stones)):\n\u001B[0;32m     55\u001B[0m     iterated_delta \u001B[38;5;241m=\u001B[39m df_sim[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime_delta_(h)\u001B[39m\u001B[38;5;124m\"\u001B[39m][i]\n\u001B[1;32m---> 56\u001B[0m     iterated_energy \u001B[38;5;241m=\u001B[39m \u001B[43mdf_sim\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEnergy_(kJ)\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     57\u001B[0m     hours_total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m iterated_delta\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (iterated_delta \u001B[38;5;241m>\u001B[39m time_interval):\n",
      "File \u001B[1;32m~\\PycharmProjects\\cwm1\\venv\\lib\\site-packages\\pandas\\core\\series.py:981\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    978\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[key]\n\u001B[0;32m    980\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[1;32m--> 981\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_hashable(key):\n\u001B[0;32m    984\u001B[0m     \u001B[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001B[39;00m\n\u001B[0;32m    985\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    986\u001B[0m         \u001B[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cwm1\\venv\\lib\\site-packages\\pandas\\core\\series.py:1089\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[1;34m(self, label, takeable)\u001B[0m\n\u001B[0;32m   1086\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[label]\n\u001B[0;32m   1088\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[1;32m-> 1089\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1090\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39m_get_values_for_loc(\u001B[38;5;28mself\u001B[39m, loc, label)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "stone_probs = []\n",
    "for i in range(10,1000000,10000):\n",
    "    print(i)\n",
    "    result = rockfall_sim(i, df_area1, df_area2)\n",
    "    stone_probs.append((result,i))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(10_000_000,50_000_000,10_000_000):\n",
    "\n",
    "    result = rockfall_sim(i, df_area1, df_area2)\n",
    "    stone_probs.append((result,i))\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#stone_probs = stone_probs[0:99]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = [i[1] for i in stone_probs]\n",
    "y = [i[0] for i in stone_probs]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"i values\")\n",
    "plt.ylabel(\"result values\")\n",
    "plt.title(\"stone_probs Plot\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "def run_sim(sim_loops,sim_count):\n",
    "    for i in range(0,sim_loops):\n",
    "        start_time = time.time()\n",
    "        final_prob = rockfall_sim(sim_count,df_area1,df_area2)\n",
    "        end_time = time.time()\n",
    "        calctime = end_time - start_time\n",
    "        sys.path.append('c:/users/logan/appdata/local/packages/pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0/localcache/local-packages/python310/site-packages')\n",
    "\n",
    "        data = {\"timestamp\": calctime,\"sim_count\": sim_count, \"final_prob\": final_prob}\n",
    "        new_row = pd.DataFrame(data, index=[0])\n",
    "        existing_df = pd.read_excel(\"Results.xlsx\")\n",
    "        appended_df = existing_df.append(new_row, ignore_index=True)\n",
    "        appended_df.to_excel(\"Results.xlsx\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_3012\\2951935073.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  appended_df = existing_df.append(new_row, ignore_index=True)\n",
      "C:\\Users\\logan\\AppData\\Local\\Temp\\ipykernel_3012\\2951935073.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  appended_df = existing_df.append(new_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "run_sim(5,10_000_000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
